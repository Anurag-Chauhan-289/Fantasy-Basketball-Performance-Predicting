{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this notebook, the ordinary least squares (OLS) method  will be used to estimate all parameters in a linear regression model. All relevant literature refers to this modeling method as \"linear regression\" and thus will be referred to as such here. \n",
    "\n",
    "This model utilizes (blank) features scraped from [basketball-reference.com](https://www.basketball-reference.com/)'s seasons tables ([totals](https://www.basketball-reference.com/leagues/NBA_2019_totals.html) and [advanced](https://www.basketball-reference.com/leagues/NBA_2019_advanced.html)) and predicts the total fantasy points a player will score the following year, otherwise referred to as __future_fantasy_points__ in the dataset. This metric was calculated following data collection according to [Yahoo's ruleset](https://help.yahoo.com/kb/fantasy-basketball/default-league-settings-fantasy-basketball-sln6919.html). \n",
    "\n",
    "It's interesting to note that every iteration of a player is considered separately. For example, 2012 LeBron James is a separate record from 2013 LeBron James. This is to say that 2012 LeBron James' __future_fantasy_points__ attribute is the same as his 2013 total fantasy points attribute, or __fantasy_points__ in the pre-processed data.\n",
    "\n",
    "While around (blank) records were scraped from basketball-reference, only (blank) records exist in the processed dataset. Consider players who were in the league for only one season. These players do not have __future_fantasy_points__ to predict and thus were dropped during the data-processing phase. Of course, the 2018-2019 season was only used to compute __future_fantasy_points__ for 2017-2018 records. See a subset of the data below.\n",
    "\n",
    "Note: All data used comes from seasons ending in (blank) to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\CA015FO\\\\basketball')\n",
    "from sqlite import get_data\n",
    "\n",
    "df = get_data('totals_advanced_nodummies_nonspecific_min41games_normalizedgames_premodel')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "The goal of a linear regression is always the same, in general, and that's to extract a random sample from a population and then to use that random sample to estimate the properties of the population. \n",
    "\n",
    "A good question to ask in terms of basketball and this model would be as follows: \"By how much does a player's fantasy points next year change for each point scored the year before?\" The answer to this question, the weight, is exactly what we're trying to optimize, along with the other (blank) features' weights. \n",
    "\n",
    "Below is the linear regression formula, where Y is __future_fantasy_points__ and, in accordance with the example above, $\\beta_1$ is the weight attributed to points scored ($x_1$). \n",
    "\n",
    "\\begin{equation*} Y = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2  +  ...  +  \\epsilon \\end{equation*}\n",
    "\n",
    "$\\beta_0$, the intercept, and $\\epsilon$, the random error, will both be touched on shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS\n",
    "The name \"ordinary least squares\" may sound like gibberish to a fresh set of ears, but it actually does a great job of briefly explaining the method's general process. OLS minimizes the sum of the squares of the differences between the observed dependent variable in the dataset and those predicted by the linear function. This is to minimize the differences between the actual __future_fantasy_points__ values and the predicted __future_fantasy_points__ values.\n",
    "\n",
    "However, the ability to perform minimization doesn't conclude the test as accurate. OLS comes with seven underlying assumptions, six of which must be met by the linear regression in order for the weights to be considered valid.\n",
    "\n",
    "Most of the assumptions attempt describe the random error, $\\epsilon$. Note the word \"attempt\" here. The random error is truly a random figure that we will never know, as it's a population value that describes the information we don't have.\n",
    "\n",
    "Thus, in order to assess $\\epsilon$ we turn to residuals, or the sample estimate of the error for each observation. Residuals can be calculated as the observed value minus the fitted value, where the observed value in this model is the actual __future_fantasy_points__ figure and the fitted value is the predicted __future_fantasy_points__ figure for a record.\n",
    "\n",
    "For the remainder of the article, I will go through each OLS assumption one by one and show whether or whether not my model satisfies the assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "x = df.iloc[:,:-1]\n",
    "x_constant = sm.add_constant(x)\n",
    "y = df['future_fantasy_points']\n",
    "model = sm.OLS(y, x_constant).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Assumption 1: The regression model is linear in the coefficients and the error term (linearity of the model)\n",
    "# Observed vs. Predicted Values should be perfectly diagonal with points equally distributed around it\n",
    "# Residuals vs. Predicted Values should be perfectly flat with points equally distributed around it\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "# def linearity_test(model, y):\n",
    "fitted_vals = model.predict()\n",
    "resids = model.resid_pearson\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color':'red'})\n",
    "ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n",
    "ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
    "\n",
    "sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color':'red'})\n",
    "ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n",
    "ax[1].set(xlabel='Predicted', ylabel='Standardized Residuals')\n",
    "ax[1].set_ylim([resids.min()-1, resids.max()+1])\n",
    "\n",
    "#     return None\n",
    "\n",
    "# linearity_test(model, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Assumption 2: The error term has a population mean of zero\n",
    "# Expectation (mean) of residuals is zero\n",
    "# Our constant makes sure this is 0\n",
    "model.resid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No (perfect) multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 100\n",
    "vif = [variance_inflation_factor(x_constant.values, i) for i in range(x_constant.shape[1])]\n",
    "vif_df = pd.DataFrame({'vif':vif[1:]}, index=x.columns)\n",
    "# vif_df[vif_df['vif'] > 1].sort_values(by='vif', ascending=False)\n",
    "# vif_df.T['field_goal_percentage']\n",
    "vif_df.sort_values(by='vif', ascending=False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.corr()\n",
    "corr_df['total_rebounds'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params['total_rebounds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pvalues[model.pvalues > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homoscedasticity (equal variance) of residuals\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "import numpy as np\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def homoscedasticity_test(model):\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "    resids_standardized = model.get_influence().resid_studentized_internal\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Residuals vs Fitted', fontsize=16)\n",
    "    ax[0].set(xlabel='Fitted Values', ylabel='Residuals')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Scale-Location', fontsize=16)\n",
    "    ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n",
    "\n",
    "    bp_test = pd.DataFrame(sms.het_breuschpagan(resids, model.model.exog), \n",
    "                           columns=['value'],\n",
    "                           index=['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value'])\n",
    "\n",
    "    gq_test = pd.DataFrame(sms.het_goldfeldquandt(resids, model.model.exog)[:-1],\n",
    "                           columns=['value'],\n",
    "                           index=['F statistic', 'p-value'])\n",
    "\n",
    "    print('\\n Breusch-Pagan test ----')\n",
    "    print(bp_test)\n",
    "    print('\\n Goldfeld-Quandt test ----')\n",
    "    print(gq_test)\n",
    "    print('\\n Residuals plots ----')\n",
    "\n",
    "homoscedasticity_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No autocorrelation of residuals\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "acf = smt.graphics.plot_acf(model.resid, lags=40 , alpha=0.05)\n",
    "acf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
